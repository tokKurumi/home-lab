#import "@preview/modern-g7-32:0.2.0": abstract, appendix-heading, appendixes, enum-numbering, gost

#set enum(numbering: enum-numbering)

#show: gost.with(
  text-size: (default: 14pt, small: 10pt),
  indent: 1.25cm,
  city: "Саранск",
  hide-title: true,
  pagination-align: center,
  margin: (
    left: 30mm,
    right: 15mm,
    top: 20mm,
    bottom: 20mm,
  ),
  add-pagebreaks: true,
)

#counter(page).update(3)

#abstract(
  "никто не читает теги",
)[
  Текст реферата
]

#outline()

= Введение

Традиционный подход к хранению данных полагается на облачные сервисы — удобные, масштабируемые, но контролируемые третьими лицами. Такая модель несёт серьёзные риски:

- *Зависимость от провайдера*. Отключение интернета, блокировка сервиса или закрытие компании означают потерю доступа к данным.
- *Отсутствие контроля над приватностью*. Облачные провайдеры хранят и обрабатывают личную информацию по своим правилам. GDPR и подобные законы лишь ограничивают, но не исключают обработку данных.
- *Постоянные утечки*. Историческое количество инцидентов безопасности показывает, что централизованные хранилища — привлекательная цель для атак.
- *Финансовая зависимость*. Цены растут, условия меняются, сервисы закрываются.

Self-hosted инфраструктура предлагает альтернативу: полный контроль над данными на собственном оборудовании, независимость от внешних сервисов (кроме интернет-провайдера), и способность адаптировать систему под конкретные нужды.

Данная работа описывает проектирование и развертывание домашней лаборатории на базе Docker, которая решает указанные проблемы через комбинацию открытого программного обеспечения, контейнеризации и принципов сетевой безопасности. Результат — полнофункциональная мультиконтейнерная инфраструктура для управления медиа-контентом, мониторинга, и безопасного удалённого доступа.

= Целевая аудитория и применение

== Для кого предназначена эта работа

Проект ориентирован на три категории пользователей:

- *Студенты и исследователи*. Изучение архитектуры распределённых систем, контейнеризации, сетевой безопасности на практическом примере.
- *Системные администраторы и DevOps инженеры*. Опыт работы с Docker Compose, конфигурированием микросервисов, управлением инфраструктурой.
- *Энтузиасты и продвинутые пользователи*. Создание собственного медиа-сервера с полным контролем, альтернатива облачным сервисам.

== Уровни сложности реализации

Инфраструктура поддерживает прогрессивное усложнение:

- *Базовая конфигурация*. Развертывание медиа-сервера (Jellyfin) и торрент-клиента (qBittorrent) с VPN защитой за 15–20 минут с использованием именованных томов Docker.
- *Промежуточная конфигурация*. Добавление обратного прокси (Nginx Proxy Manager), локальной DNS системы, интеграция с собственным доменом через Cloudflare.
- *Расширенная конфигурация*. Полная автоматизация медиа-контента (Sonarr, Radarr, Lidarr), обработка файлов (FileFlows), мониторинг (Prometheus, Grafana), безопасный удалённый доступ через VPN.

== Основные возможности

Инфраструктура включает набор интегрированных сервисов, каждый из которых решает определённую задачу в экосистеме домашней лаборатории. От управления медиа-контентом до обеспечения безопасного удалённого доступа — система охватывает полный цикл работы с данными. Полный перечень функциональности и соответствующих компонентов представлен в таблице @capabilities-table, которая демонстрирует разнообразие возможностей и гибкость архитектуры.

#figure(
  table(
    columns: 3,
    table.header([Возможность], [Назначение], [Используемые сервисы]),
    [Управление медиа],
    [Автоматизация скачивания контента, управление индексерами и обработка запросов пользователей],
    [Sonarr, Radarr, Lidarr, Prowlarr, Jellyseerr],

    [Потоковая передача],
    [Воспроизведение медиа на различных устройствах, интеграция с Discord],
    [Jellyfin, Discord Music Bot],

    [Защищённое скачивание], [Торрент-загрузки через VPN для приватности], [qBittorrent, WireGuard],
    [Транскодирование], [Конвертация видео и аудио в оптимальные форматы], [FileFlows],
    [Обратное прокси], [Единая точка входа для всех сервисов с SSL/TLS], [Nginx Proxy Manager],
    [Мониторинг инфраструктуры], [Сбор метрик и визуализация системы], [Prometheus, Grafana, Node Exporter],
    [DNS и управление доменами],
    [Локальное разрешение имён с DOH/TOH и интеграция с внешним доменом],
    [AdGuardHome, Cloudflare],

    [Удалённый доступ], [Безопасное подключение с домашней сети извне], [WireGuard VPN],
  ),
  caption: [Основные возможности домашней лаборатории],
) <capabilities-table>

= Архитектура инфраструктуры

== Обзор компонентов

Система состоит из 14 основных сервисов, организованных по функциональным блокам. Каждый компонент работает в изолированном контейнере Docker и взаимодействует с другими через виртуальные сети и API.

#figure(
  table(
    columns: 4,
    table.header([Сервис], [Функция], [Порт], [Блок]),
    [#link("https://github.com/Fallenbagel/jellyseerr")[Jellyseerr]], [Интерфейс запросов], [5055], [Управление медиа],
    [#link("https://sonarr.tv/")[Sonarr]], [Автоматизация сериалов], [8989], [Управление медиа],
    [#link("https://radarr.video/")[Radarr]], [Автоматизация фильмов], [7878], [Управление медиа],
    [#link("https://lidarr.audio/")[Lidarr]], [Автоматизация музыки], [8686], [Управление медиа],
    [#link("https://prowlarr.com/")[Prowlarr]], [Управление индексерами], [9696], [Управление медиа],
    [#link("https://jellyfin.org/")[Jellyfin]], [Потоковая передача медиа], [8096], [Потоковая передача],
    [#link("https://github.com/manuel-rw/jellyfin-discord-music-bot")[Discord Music Bot]],
    [Воспроизведение музыки в Discord],
    [3000],
    [Потоковая передача],

    [#link("https://www.qbittorrent.org/")[qBittorrent]], [Торрент-клиент (через VPN)], [8080], [Загрузки],
    [#link("https://github.com/revenz/FileFlows")[FileFlows]], [Транскодирование видео], [5000], [Обработка файлов],
    [#link("https://www.wireguard.com/")[WireGuard]], [VPN сервер], [51820], [Сеть],
    [#link("https://adguard.com/adguard-home/home.html")[AdGuardHome]], [DNS с DOH/TOH], [53, 853, 443], [Сеть],
    [#link("https://nginxproxymanager.com/")[Nginx Proxy Manager]], [Обратный прокси + SSL], [80, 81, 443], [Сеть],
    [#link("https://www.cloudflare.com/")[Cloudflare]], [DNS и управление доменом], [—], [Сеть],
    [#link("https://prometheus.io/")[Prometheus]], [Сбор метрик], [9090], [Мониторинг],
    [#link("https://grafana.com/")[Grafana]], [Визуализация графиков], [3000], [Мониторинг],
    [#link("https://github.com/prometheus/node_exporter")[Node Exporter]], [Метрики хоста], [9100], [Мониторинг],
    [#link("https://www.portainer.io/")[Portainer]],
    [Управление Docker контейнерами],
    [8000, 9000, 9443],
    [Инфраструктура],

    [#link("https://github.com/dani-garcia/vaultwarden")[Vaultwarden]], [Хранилище паролей], [80], [Инфраструктура],
    [#link("https://homarr.dev/")[Homarr (Dashboard)]], [Главная страница и управление], [7575], [Инфраструктура],
    [#link("https://openspeedtest.com/")[OpenSpeedTest]],
    [Тестирование скорости интернета],
    [3000-3001, 8080],
    [Инфраструктура],

    [#link("https://containrrr.dev/watchtower/")[Watchtower]],
    [Автоматическое обновление контейнеров],
    [—],
    [Инфраструктура],
  ),
  caption: [Компоненты системы и их функции],
) <components-table>

== Сетевая архитектура

=== Физическая сеть домашней лаборатории

Перед рассмотрением виртуальных сетей Docker необходимо описать физическую инфраструктуру, в которой развёрнута система:

- *Роутер на базе OpenWrt* — маршрутизирует весь трафик домашней сети, управляет правилами firewall, обеспечивает базовую DNS инфраструктуру.

- *Управляемый L3 коммутатор* — обеспечивает физическую связность устройств в сети. Используется VLAN сегментация, однако описана она не будет, так как выходит за рамки простой пользовательской конфигурации. При желании, энтузиаст-читатель этой курсовой работы может изучить этот вопрос самостоятельно.

- *Firewall с маркировкой трафика* — реализован на роутере OpenWrt. Использует списки доменов и CIDR диапазонов для маркировки трафика. Помеченные ресурсы автоматически направляются в VPN туннель через sign-box. Списки для маршрутизации берутся из проекта #link("https://podkop.net/")[podkop.net].

- *Каскадная DNS инфраструктура* — трёхуровневая система разрешения имён:
  1. *dnsmasq* (базовый DNS роутера) — первичная обработка запросов, кеширование, локальные записи
  2. *sign-box DNS* — маркировка доменов для firewall, определение маршрутизации через VPN
  3. *AdGuardHome* — фильтрация рекламного трафика, DOH/DOT к публичным провайдерам

Конфигурация AdGuardHome использует балансировку между несколькими зашифрованными DNS провайдерами:

#figure(
  ```yaml
  # Основные DNS с шифрованием (DOT/DOH)
  upstream:
    - https://dns10.quad9.net/dns-query
    - https://cloudflare-dns.com/dns-query
    - https://dns.google/dns-query
    - tls://dns10.quad9.net
    - tls://1.1.1.1
    - tls://dns.google

  # Bootstrap DNS (для разрешения имён основных DNS)
  bootstrap-dns:
    - 9.9.9.10          # Quad9 (основной)
    - 149.112.112.10    # Quad9 (резервный)
    - 2620:fe::10       # Quad9 IPv6
    - 2620:fe::fe:10    # Quad9 IPv6 резервный

  # Fallback DNS (незашифрованные, крайний случай)
  fallback-dns:
    - 1.1.1.1           # Cloudflare
    - 1.0.0.1           # Cloudflare резервный
    - 8.8.8.8           # Google DNS
    - 8.8.4.4           # Google DNS резервный
  ```,
  caption: [Конфигурация DNS балансировщика AdGuardHome],
) <adguard-dns-config>

Такая архитектура обеспечивает отказоустойчивость, приватность через шифрование DNS запросов и автоматическую маршрутизацию специфичного трафика через VPN.

=== Виртуальные сети Docker

Каждый функциональный блок инфраструктуры изолирован в собственной виртуальной сети Docker. Контейнеры внутри одного блока видят друг друга и могут взаимодействовать напрямую по имени хоста, что обеспечивает логическую организацию.

Все остальные виртуальные сети в рамках Docker контейнеров:

- *media-network* — сеть для всех медиа-сервисов (Jellyfin, Sonarr, Radarr, Lidarr, Prowlarr, Jellyseerr, qBittorrent, FileFlows). Обеспечивает взаимодействие между компонентами автоматизации и потоковой передачи.
- *proxiable* — публичная сеть для сервисов, доступных через Nginx Proxy Manager. Позволяет обратному прокси маршрутизировать запросы к нужным контейнерам.
- *monitoring* — выделенная сеть для системы мониторинга (Prometheus, Grafana, Node Exporter). Изолирует сбор метрик от основной инфраструктуры.
- *vpn* — сеть для VPN контейнера WireGuard. Используется qBittorrent для приватных скачиваний через зашифрованное соединение.

Такая организация позволяет контейнерам обращаться друг к другу по имени (например, `http://sonarr:8989`), исключая необходимость в фиксированных IP адресах и упрощая конфигурацию.

== Взаимодействие контейнеров

Инфраструктура содержит множество потоков взаимодействия между сервисами. Ниже описан один из самых сложных флоу — полная автоматизация загрузки и каталогизации медиа-контента, который начинается с пользовательского интерфейса.

- Пользователь подключается к *Nginx Proxy Manager* — единой точке входа для всех интерактивных сервисов. Прокси маршрутизирует запрос к *Jellyseerr* в виртуальной Docker сети.
- Пользователь через *Jellyseerr* отправляет запрос на фильм или сериал
- Запрос автоматически создаётся в *Sonarr* (сериалы) или *Radarr* (фильмы) через REST API
- *Sonarr* / *Radarr* отправляют поисковый запрос в *Prowlarr* (агрегатор индексеров торрентов) для поиска торренто-ссылок
- *Sonarr* / *Radarr* отправляют найденные торренты в *qBittorrent* через REST API для загрузки
- *qBittorrent* работает в виртуальной сети *WireGuard* для обеспечения приватности скачиваний
- Загруженные файлы сохраняются в общем хранилище (*File Storage*)
- *FileFlows* автоматически обрабатывает (транскодирует) видео файлы при необходимости
- *Jellyfin* периодически сканирует хранилище и каталогизирует готовый контент, делая его доступным пользователям через сеть

#figure(
  image("diagrams/media-flow.png", width: 100%),
  caption: [Взаимодействия контейнеров автоматизации медиа-контента],
) <media-flow-diagram>

Помимо основного потока, инфраструктура содержит ряд автоматических процессов, недоступных для прямого пользовательского управления, а также простых связок взаимодействий между сервисом и обратным прокси:

- *Watchtower* — мониторит сокет демона Docker, периодически проверяет наличие обновлений для образов контейнеров и автоматически перезапускает сервисы при наличии новых версий. Такой подход позволяет содержать инфраструктуру в актуальном состоянии без ручного вмешательства.

- *Node Exporter* и *cAdvisor* — собирают метрики системы и контейнеров в реальном времени, отправляя их в *Prometheus* для хранения и анализа. *Grafana* визуализирует эти метрики в виде графиков и дашбордов.

- *Публично доступные сервисы* — ряд компонентов (*Jellyfin*, *Vaultwarden*, *Homarr* и другие) доступны конечному пользователю напрямую через их HTTP/HTTPS порты, проксированные *Nginx Proxy Manager*.

= Выбор оборудования

== Профиль потребления ресурсов

Перед развертыванием инфраструктуры необходимо понимать реальные требования к оборудованию. В отличие от облачных сервисов, где потребление зачастую не волнует нас, домашняя лаборатория требует явного расчёта ресурсов.

=== Потребление оперативной памяти

В состоянии покоя (без активных пользователей и обработки) все сервисы инфраструктуры в сумме занимают не более *6 гигабайт оперативной памяти*. Это достаточно скромный объём, что позволяет развертывать систему на оборудовании с 8–16 ГБ ОЗУ.

Основные потребители:

- *FileFlows* (транскодирование). По умолчанию содержит 3 параллельных ранера (worker) для одновременной обработки видео. При транскодировании видеоконтента, даже 4K с высоким битрейтом (80 тыс кбит/с), один ранер занимает примерно *1 гигабайт памяти*, однако нагрузка на центральный процессор отсутствует полностью — вся работа выполняется на видеокарте через NVIDIA NVENC/NVDEC ускорители.

- *Jellyfin* (потоковая передача). Аналогично FileFlows использует NVIDIA GPU ускорители для транскодирования и имеет сопоставимое потребление памяти (около 1 ГБ за сеанс при транскодировании). Оба сервиса используют единый CLI плагин FFmpeg, что обеспечивает идентичное поведение и потребление ресурсов. *Важный момент*: Jellyfin разумно избегает ненужного транскодирования — если контент представлен в кодеке, который может аппаратно воспроизвести клиент (например, HEVC на современных мобильных устройствах), транскодирование не происходит вообще. Дополнительно, Jellyfin кеширует результаты транскодирования, поэтому если множество пользователей одновременно смотрят один и тот же фильм, видеокодек "прогоняется" только один раз, остальные получают кеш из хранилища.

- *Grafana* (мониторинг) и *Homarr* (дашбоард). Эти сервисы могут занимать значительный объём памяти (до 500 МБ–1 ГБ) вследствие сложного пользовательского интерфейса.

- *Остальные сервисы*. Nginx Proxy Manager просто маршрутизирует трафик и периодически обновляет SSL сертификаты — занимает минимум памяти. Watchtower мониторит сокет демона Docker, периодически проверяет обновления образов и перезапускает контейнеры — также не требователен к ресурсам. Медиа-сервисы (Sonarr, Radarr, Lidarr, Prowlarr, Jellyseerr) выполняют лишь API запросы и управление метаданными — их потребление пренебрежимо мало.

=== Потребление процессора

Инфраструктура всех приложений спроектирована таким образом, чтобы минимизировать нагрузку на центральный процессор. Большинство операций либо асинхронные (проверка обновлений, сбор метрик), либо
вовсе I/O-bound (работа с диском, сетью). #underline[Единственным исключением является видеотранскодирование, которое полностью разгружено на видеокарту].

Таким образом, процессор может быть относительно скромным — достаточно 4–6 ядер для обеспечения параллельной работы контейнеров и фоновых задач. При использовании GPU ускорения нагрузка на CPU при транскодировании практически отсутствует.

=== Требования к хранилищу

Хранилище состоит из двух компонентов:

- *Системный диск* (для Docker и конфигурации). Требуется минимум 100–150 ГБ SSD для хранения образов контейнеров, конфигурационных файлов и баз данных сервисов.

- *Медиа-хранилище* (для контента). Объём зависит от размера коллекции фильмов, сериалов и музыки. Для комфортного использования рекомендуется от 1–2 ТБ и выше. Тип хранилища может быть как SSD (для максимальной производительности), так и HDD (для экономии бюджета, так как медиа в основном читается последовательно).

== NVIDIA GPU

Если планируется транскодирование видеоконтента или использование аппаратного ускорения потоковой передачи, рекомендуется видеокарта NVIDIA с поддержкой CUDA.

Требования:

- Видеокарта: NVIDIA GeForce GTX 1050 и выше (поддержка NVENC/NVDEC)
- Драйверы: NVIDIA Driver 450.x и выше

Cистема полностью функциональна и без видеокарты, однако транскодирование видео будет выполняться на процессоре, что может быть медленным при обработке множественных потоков или 4K контента.
